version: '2.3'
services:
  serving:
    image: "tensorflow/serving:1.12.0-gpu"
    restart: always
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-all}
    volumes:
      - ${FTI_IDCARD_EXPORT_PATH:-./export}:/share/export:z
      - ${FTI_IDCARD_SERVING_CONFIG:-./resources/configs/tensorflow}:/share/config:z
      - ./resources:/code/resources
    ports:
      - "${FTI_IDCARD_SERVING_PORT:-8500}:8500"
    command: ["tensorflow_model_server", 
                  "--model_config_file=/share/config/serving.config", 
                  "--enable_model_warmup=true",
                  "--per_process_gpu_memory_fraction=$GPU_MEMORY_FRACTION"]
  api:
    image: "gcr.io/fti-vision/ftid-api:latest"
    restart: always
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./resources:/code/resources
      - ./logs:/code/logs
    environment:
      - FTI_IDCARD_WORKERS=${FTI_IDCARD_WORKERS:-4}
      - FTI_IDCARD_SERVING_HOST=serving:8500
      - FRONT_PRIORITY=${FRONT_PRIORITY:-0}
      - SAVE_DEBUG_IMAGES=${SAVE_DEBUG_IMAGES:-1}
      - SAVE_NO_CARD_IMAGES=${SAVE_NO_CARD_IMAGES:-1}
      - ISSUE_LOC_NEW_BACK=${ISSUE_LOC_NEW_BACK:-1}
      - HOST_NAME=${HOSTNAME:-hostname}
    ports:
      - "${FTI_IDCARD_PORT:-5000}:5000"
